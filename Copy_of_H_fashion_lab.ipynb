{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ec02c415-becb-4625-bd39-7eea8a052bb2",
      "metadata": {
        "id": "ec02c415-becb-4625-bd39-7eea8a052bb2"
      },
      "source": [
        "# G:  Fashion"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "283bc0ba-e885-4b81-8e2f-3326e80d3464",
      "metadata": {
        "id": "283bc0ba-e885-4b81-8e2f-3326e80d3464"
      },
      "source": [
        "Zalando - the online fashion retailer - has created a dataset. It is available on Kaggle but, like MNIST, Keras keeps a copy. It is deliberately similar to MNIST (which is why the dataset is called Fashion-MNIST). It comprises 70,000 grayscale images, each one $28 \\times 28$ pixels in size. 60,000 are training examples and 10,000 are test examples. Each is labelled with one of 10 classes, already Label Encoded 0-9, where the labels represent the following: 0 T-shirt/top, 1 Trouser, 2 Pullover, 3 Dress, 4 Coat, 5 Sandal, 6 Shirt, 7 Sneaker, 8 Bag, 9 Ankle boot.\n",
        "\n",
        "In this lab, I want you to get used to coding up neural networks. Build some models using Keras; and try some model selection using keras-tuner.\n",
        "\n",
        "There are neither hints nor answers in this lab sheet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9095a9f5-2e9d-4d24-821f-25e479a2fb38",
      "metadata": {
        "id": "9095a9f5-2e9d-4d24-821f-25e479a2fb38"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-06 21:47:14.369467: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-06 21:47:14.370084: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-11-06 21:47:14.372961: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-11-06 21:47:14.380234: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762465634.392313   80794 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762465634.395674   80794 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762465634.405074   80794 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762465634.405092   80794 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762465634.405094   80794 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762465634.405096   80794 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-06 21:47:14.408599: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "# Here is a selection of import statements.\n",
        "# You might not use all of them; and you might decide to include extra ones.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras import Model\n",
        "from keras import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Rescaling\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Dropout\n",
        "import keras\n",
        "from keras import layers\n",
        "\n",
        "from keras import Model\n",
        "from keras import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Normalization\n",
        "from keras.layers import Rescaling\n",
        "\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "from keras.datasets import mnist\n",
        "\n",
        "from keras.models import load_model\n",
        "\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from keras.regularizers import l2\n",
        "\n",
        "from keras.datasets import fashion_mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "aa5796fd-0da3-40d5-b54f-e7dfd7ce3d7e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa5796fd-0da3-40d5-b54f-e7dfd7ce3d7e",
        "outputId": "25a7f193-cf6f-4f53-d12a-20f970d18564"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras_tuner in ./.ml-venv/lib/python3.12/site-packages (1.4.7)\n",
            "Requirement already satisfied: keras in ./.ml-venv/lib/python3.12/site-packages (from keras_tuner) (3.12.0)\n",
            "Requirement already satisfied: packaging in ./.ml-venv/lib/python3.12/site-packages (from keras_tuner) (25.0)\n",
            "Requirement already satisfied: requests in ./.ml-venv/lib/python3.12/site-packages (from keras_tuner) (2.32.5)\n",
            "Requirement already satisfied: kt-legacy in ./.ml-venv/lib/python3.12/site-packages (from keras_tuner) (1.0.5)\n",
            "Requirement already satisfied: absl-py in ./.ml-venv/lib/python3.12/site-packages (from keras->keras_tuner) (2.3.1)\n",
            "Requirement already satisfied: numpy in ./.ml-venv/lib/python3.12/site-packages (from keras->keras_tuner) (2.1.3)\n",
            "Requirement already satisfied: rich in ./.ml-venv/lib/python3.12/site-packages (from keras->keras_tuner) (14.2.0)\n",
            "Requirement already satisfied: namex in ./.ml-venv/lib/python3.12/site-packages (from keras->keras_tuner) (0.1.0)\n",
            "Requirement already satisfied: h5py in ./.ml-venv/lib/python3.12/site-packages (from keras->keras_tuner) (3.15.1)\n",
            "Requirement already satisfied: optree in ./.ml-venv/lib/python3.12/site-packages (from keras->keras_tuner) (0.17.0)\n",
            "Requirement already satisfied: ml-dtypes in ./.ml-venv/lib/python3.12/site-packages (from keras->keras_tuner) (0.5.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in ./.ml-venv/lib/python3.12/site-packages (from requests->keras_tuner) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.ml-venv/lib/python3.12/site-packages (from requests->keras_tuner) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.ml-venv/lib/python3.12/site-packages (from requests->keras_tuner) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.ml-venv/lib/python3.12/site-packages (from requests->keras_tuner) (2025.10.5)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in ./.ml-venv/lib/python3.12/site-packages (from optree->keras->keras_tuner) (4.15.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.ml-venv/lib/python3.12/site-packages (from rich->keras->keras_tuner) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.ml-venv/lib/python3.12/site-packages (from rich->keras->keras_tuner) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in ./.ml-venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras->keras_tuner) (0.1.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# If you are running on Google Colab, uncomment the next line before executing this code cell.\n",
        "\n",
        "! pip install keras_tuner\n",
        "\n",
        "import keras_tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a9c3927e-d831-4199-8993-22fbd7a34a19",
      "metadata": {
        "id": "a9c3927e-d831-4199-8993-22fbd7a34a19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0366926e-9513-4a7f-ab34-47f5497fdf2d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "0366926e-9513-4a7f-ab34-47f5497fdf2d",
        "outputId": "60d38435-1ad8-49cf-bed5-8b011c7f4a67"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7455cb9bc620>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.imshow(X_train[20], cmap=plt.cm.binary, interpolation=\"nearest\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56a75904-67c7-4a88-885f-9b6c838eed9c",
      "metadata": {
        "id": "56a75904-67c7-4a88-885f-9b6c838eed9c"
      },
      "source": [
        "## Dense Network"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d6be2da-4790-4797-8754-f79a141fe45e",
      "metadata": {
        "id": "8d6be2da-4790-4797-8754-f79a141fe45e"
      },
      "source": [
        "We'll start with a dense network: one with *no* convolutional layers.\n",
        "\n",
        "- Reshape the images so that each becomes one-dimensonal.\n",
        "- Then define a dense neural network: input layer; rescaling; one dense hidden layer with 512 units using ReLU; and a dense output layer using the appropriate activation function.\n",
        "- Compile it: use RMSprop as the optimizer with a learning rate of 0.01; choose the appropriate loss function; use accuracy as the metric.\n",
        "- Fit it: use 0.25 validation split; 10 epochs; batch size of 32.\n",
        "- Look at the training accuracy and validation accuracy: do you think you are underfitting? overfitting?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vSP1v3ED-HO7",
      "metadata": {
        "id": "vSP1v3ED-HO7"
      },
      "source": [
        "# Reshape image\n",
        "X_train = X_train.reshape((60000, 28 * 28))\n",
        "X_test = X_test.reshape((10000, 28 * 28))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "MNEA7mst_Ejj",
      "metadata": {
        "id": "MNEA7mst_Ejj"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1762465643.586623   80794 cuda_executor.cc:1228] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\n",
            "W0000 00:00:1762465643.589461   80794 gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n"
          ]
        }
      ],
      "source": [
        "# Define dense layers\n",
        "inputs = Input(shape=(784,))\n",
        "x = Normalization()(inputs)\n",
        "x = Dense(units=512, activation=\"relu\")(x)\n",
        "outputs = Dense(units=10, activation=\"softmax\")(x)\n",
        "cloth_model = Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "mnsp_DhPAVA9",
      "metadata": {
        "id": "mnsp_DhPAVA9"
      },
      "outputs": [],
      "source": [
        "# Compile it\n",
        "cloth_model.compile(optimizer=RMSprop(learning_rate=0.01),\n",
        "                    loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbYg6acpBEOF",
      "metadata": {
        "id": "dbYg6acpBEOF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Input 0 of layer \"functional\" is incompatible with the layer: expected shape=(None, 784), found shape=(None, 28, 28)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m history = \u001b[43mcloth_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/colab/homl_2025_2026/.ml-venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/colab/homl_2025_2026/.ml-venv/lib/python3.12/site-packages/keras/src/layers/input_spec.py:245\u001b[39m, in \u001b[36massert_input_compatibility\u001b[39m\u001b[34m(input_spec, inputs, layer_name)\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    244\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim != dim:\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    246\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of layer \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m is \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    247\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mincompatible with the layer: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    248\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    249\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    250\u001b[39m         )\n",
            "\u001b[31mValueError\u001b[39m: Input 0 of layer \"functional\" is incompatible with the layer: expected shape=(None, 784), found shape=(None, 28, 28)"
          ]
        }
      ],
      "source": [
        "# history = cloth_model.fit(\n",
        "#     X_train, y_train, validation_split=0.25, epochs=10, batch_size=32, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70f17ad0-d801-4fde-9490-2b0f4cc98743",
      "metadata": {
        "id": "70f17ad0-d801-4fde-9490-2b0f4cc98743"
      },
      "outputs": [],
      "source": [
        "# train_acc, val_acc = history.history[\"accuracy\"][-1], history.history[\"val_accuracy\"][-1]\n",
        "# train_acc, val_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "Honc_IzHB-e6",
      "metadata": {
        "id": "Honc_IzHB-e6"
      },
      "outputs": [],
      "source": [
        "def plot_keras_history(history, metric):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
        "    fig.tight_layout()\n",
        "    axes[0].plot(history.history[\"loss\"], label=\"train loss\")\n",
        "    axes[0].plot(history.history[\"val_loss\"], label=\"val loss\")\n",
        "    axes[0].set_title(\"Loss\")\n",
        "    axes[0].legend()\n",
        "    axes[1].plot(history.history[metric], label=\"train \" + metric)\n",
        "    axes[1].plot(history.history[\"val_\" + metric], label=\"val \" + metric)\n",
        "    axes[1].set_title(metric)\n",
        "    axes[1].legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RWShjf8GCNOQ",
      "metadata": {
        "id": "RWShjf8GCNOQ"
      },
      "outputs": [],
      "source": [
        "plot_keras_history(history, \"accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74cce85f",
      "metadata": {},
      "source": [
        "TESTING COLAB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "H-MpleTiDJ3m",
      "metadata": {
        "id": "H-MpleTiDJ3m"
      },
      "outputs": [],
      "source": [
        "def build_model(hp):\n",
        "    # inputs = Input(shape=(4,))\n",
        "    # x = Normalization()(inputs)\n",
        "    # x = Dense(units=16, activation=\"relu\")(x)\n",
        "    # hp_is_multilayered = hp.Boolean(\"is_multi_layered\")\n",
        "    # if hp_is_multilayered:\n",
        "    #     x = Dense(hp.Choice(\"units\", [2, 4, 8]), activation=\"relu\")(x)\n",
        "    # outputs = Dense(units=1, activation=\"linear\")(x)\n",
        "    # housing_model = Model(inputs, outputs)\n",
        "    # housing_model.compile(optimizer=hp.Choice(\"optimizer\", values =[\"sgd\", \"rmsprop\", \"adam\", \"nadam\"]),\n",
        "    #                       loss=\"mse\", metrics=[\"mae\"])\n",
        "    inputs = Input(shape=(28*28,))\n",
        "    x = Normalization()(inputs)\n",
        "    \n",
        "    x = Dense(units=10,activation=\"softmax\")\n",
        "    model = Model(input, outputs)\n",
        "    model.compile(optimizer=hp.Choice(\"optimizer\", values=[\"rmsprop\",\"adam\"]), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vPFdZvbeQVmm",
      "metadata": {
        "id": "vPFdZvbeQVmm"
      },
      "outputs": [],
      "source": [
        "# def build_model(hp):\n",
        "#     inputs = Input(shape=(28*28,))\n",
        "#     x = Rescaling(scale-1./255)(inputs)\n",
        "#     x = Dense(units=16, activation=\"relu\")(x)\n",
        "#     hp_is_multilayered = hp.Boolean(\"is_multi_layered\")\n",
        "#     if hp_is_multilayered:\n",
        "#         x = Dense(hp.Choice(\"units\", [2, 4, 8]), activation=\"relu\")(x)\n",
        "#     outputs = Dense(units=1, activation=\"linear\")(x)\n",
        "#     housing_model = Model(inputs, outputs)\n",
        "#     housing_model.compile(optimizer=hp.Choice(\"optimizer\", values =[\"sgd\", \"rmsprop\", \"adam\", \"nadam\"]),\n",
        "#                           loss=\"mse\", metrics=[\"mae\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sOagcWARIyeh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOagcWARIyeh",
        "outputId": "b9075c8a-1e0a-4ad2-994a-d2835b1e2b8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Search space summary\n",
            "Default search space size: 5\n",
            "num_layers (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 4, 'step': 1, 'sampling': 'linear'}\n",
            "units_1 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 4, 'max_value': 16, 'step': 4, 'sampling': 'linear'}\n",
            "optimizer (Choice)\n",
            "{'default': 'rmsprop', 'conditions': [], 'values': ['rmsprop', 'adam'], 'ordered': False}\n",
            "units_2 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 4, 'max_value': 16, 'step': 4, 'sampling': 'linear'}\n",
            "units_3 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 4, 'max_value': 16, 'step': 4, 'sampling': 'linear'}\n"
          ]
        }
      ],
      "source": [
        "build_model(keras_tuner.HyperParameters())\n",
        "tuner.search_space_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BADB_oSnIcl7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BADB_oSnIcl7",
        "outputId": "6c89f3dc-c111-412f-927b-f3f3c78d22a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 10 Complete [00h 00m 46s]\n",
            "val_accuracy: 0.5763999819755554\n",
            "\n",
            "Best val_accuracy So Far: 0.6403999924659729\n",
            "Total elapsed time: 00h 07m 53s\n"
          ]
        }
      ],
      "source": [
        "tuner = keras_tuner.RandomSearch(\n",
        "    build_model,\n",
        "    objective=\"val_accuracy\",\n",
        "    max_trials=10,\n",
        "    directory=\".\",\n",
        "    project_name=\"tuner_state\",\n",
        "    overwrite=True)\n",
        "# tuner.search_space_summary()\n",
        "tuner.search(X_train, y_train, epochs=10, validation_split=0.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XRB5zSFwOgJi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRB5zSFwOgJi",
        "outputId": "08ba5a04-b82d-4f80-f180-40c06a5570b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'num_layers': 4,\n",
              " 'units_1': 12,\n",
              " 'optimizer': 'adam',\n",
              " 'units_2': 12,\n",
              " 'units_3': 16}"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tuner.get_best_hyperparameters()[0].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rSxOgwWLOk4p",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSxOgwWLOk4p",
        "outputId": "5f3c160b-e01c-46c7-f03f-c0194a81a6db"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        }
      ],
      "source": [
        "best_model = tuner.get_best_models(num_models=1)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PjzzgRo_K9Ja",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "PjzzgRo_K9Ja",
        "outputId": "de813a0b-2ba2-4206-d933-76e4d79f5468"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,420</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">156</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">208</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">170</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m)             │         \u001b[38;5;34m9,420\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m)             │           \u001b[38;5;34m156\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m208\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m170\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,954</span> (38.88 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,954\u001b[0m (38.88 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,954</span> (38.88 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,954\u001b[0m (38.88 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "best_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ttPrMK22ObUk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttPrMK22ObUk",
        "outputId": "6a8eed17-23b3-4281-d158-f4749ae34d29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.6285 - loss: 0.8472 - val_accuracy: 0.5845 - val_loss: 1.4891\n",
            "Epoch 2/10\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.6279 - loss: 0.8817 - val_accuracy: 0.6433 - val_loss: 0.8327\n",
            "Epoch 3/10\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.6291 - loss: 0.8339 - val_accuracy: 0.6435 - val_loss: 0.8318\n",
            "Epoch 4/10\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.6349 - loss: 0.8220 - val_accuracy: 0.6197 - val_loss: 0.8981\n",
            "Epoch 5/10\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.6313 - loss: 0.8348 - val_accuracy: 0.6337 - val_loss: 0.8594\n",
            "Epoch 6/10\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.6415 - loss: 0.8164 - val_accuracy: 0.6422 - val_loss: 0.8357\n",
            "Epoch 7/10\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.6462 - loss: 0.7981 - val_accuracy: 0.6407 - val_loss: 0.8336\n",
            "Epoch 8/10\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.6394 - loss: 0.8267 - val_accuracy: 0.6246 - val_loss: 0.9413\n",
            "Epoch 9/10\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.6403 - loss: 0.8302 - val_accuracy: 0.6395 - val_loss: 0.8450\n",
            "Epoch 10/10\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.6403 - loss: 0.8281 - val_accuracy: 0.6451 - val_loss: 0.8302\n"
          ]
        }
      ],
      "source": [
        "history = best_model.fit(\n",
        "    X_train, y_train, validation_split=0.25, epochs=10, batch_size=32, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7NmtYgSXO1AA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NmtYgSXO1AA",
        "outputId": "779d49db-500c-4d64-d63f-dd7e4113bed6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.6388000249862671, 0.6450666785240173)"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_acc, val_acc = history.history[\"accuracy\"][-1], history.history[\"val_accuracy\"][-1]\n",
        "train_acc, val_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fec1ee6-21d2-4db8-a2ce-a162f7d0ac28",
      "metadata": {
        "id": "0fec1ee6-21d2-4db8-a2ce-a162f7d0ac28"
      },
      "source": [
        "Now, let's practice with `keras-tuner` hyperparameters.\n",
        "\n",
        "Define a function that builds and compiles the model. Use `keras-tuner` hyperparameters so\n",
        "- the network may have one dense hidden layer, or two or three\n",
        "- the number of neurons in each of these layers may be 4 or 8 or 16\n",
        "- the optimizer may be RMsprop or Adam\n",
        "\n",
        "Use a random search, rather than grid search, with a maximum of 10 trials.\n",
        "\n",
        "What are the best hyperparameter values?\n",
        "\n",
        "Then fit a model using these hyperaparameter values and get the training and validation accuracy. Are they better than for the model you defined above? Are we underfitting? Overfitting?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b52d4430-5c61-446a-8450-f910e098cd07",
      "metadata": {
        "id": "b52d4430-5c61-446a-8450-f910e098cd07"
      },
      "source": [
        "## Convolutional Network"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ca3a6a7-d738-4643-b563-31871681247d",
      "metadata": {
        "id": "5ca3a6a7-d738-4643-b563-31871681247d"
      },
      "source": [
        "Now we'll try a convolutional neural network. We won't use `keras-tuner` this time - it would all take too long.\n",
        "\n",
        "- Reshape the images so that each becomes three-dimensional.\n",
        "- Then define a convolutional neural network: input layer; recsaling; a convolutional layer with a $3 \\times 3$ window, 8 feature maps, and using ReLU; batch normalization; max pooling with a $2 \\times 2$ window; then another convolutional layer, this time with 4 feature maps; batch normalization; max pooling; then flatten; and a dense output layer using the appropriate activation function.\n",
        "- Compile it: use RMSprop as the optimizer with a learning rate of 0.01; choose the appropriate loss function; use accuracy as the metric.\n",
        "- Fit it: use 0.25 validation split; 30 epochs; batch size of 32. But this time include early stopping with a patience of 4.\n",
        "  \n",
        "Look at the training accuracy and validation accuracy. Is this better than the models you defined earlier? Are we underfitting? Overfitting?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98f14dc2-7d1a-49bf-9b6f-e68a1c4e2733",
      "metadata": {
        "id": "98f14dc2-7d1a-49bf-9b6f-e68a1c4e2733"
      },
      "source": [
        "Suppose I tell you that people claim that accuracy of at least 93% and even up to about 99% is possible on this dataset.\n",
        "\n",
        "Hence, are your models underfitting? Overfitting? Neither?\n",
        "\n",
        "Within the time you have available, change your convolutional network - try to get training accuracy and validation accuracy above 92%. Call me over to show me any notable successes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae44c0c3-88c8-4f04-ba09-9288e018f4e3",
      "metadata": {
        "id": "ae44c0c3-88c8-4f04-ba09-9288e018f4e3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".ml-venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
