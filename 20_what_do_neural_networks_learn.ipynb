{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eu14d9EV4oTO"
   },
   "source": [
    "# What Do Neural Networks Learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zSWUjCG34oTP"
   },
   "source": [
    "You are advised to run this Jupyter Notebook on Google Colab. From the Colab toolbar, select *Runtime* > *Change runtime type* > *T4 GPU* > *Save* before running the Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1755852015511,
     "user": {
      "displayName": "Derek Bridge",
      "userId": "01707280039117110470"
     },
     "user_tz": -60
    },
    "id": "kfu9IlEl4oTQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.applications import Xception\n",
    "import keras.applications.xception as xception\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "from keras import Model\n",
    "from keras import Input\n",
    "from keras.layers import Rescaling\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.utils import load_img, img_to_array, array_to_img, save_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The third demo in this Notebook requires an extra Python library: opencv-python\n",
    "# Installing it on your own machine may downgrade your version of numpy - which we don't want.\n",
    "# So it is is better to run this Notebook on Google Colab.\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1692,
     "status": "ok",
     "timestamp": 1755852017205,
     "user": {
      "displayName": "Derek Bridge",
      "userId": "01707280039117110470"
     },
     "user_tz": -60
    },
    "id": "RaowgSAF4oTQ",
    "outputId": "25a7614b-870e-411b-cdae-70840b228a02"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  base_dir = \"./drive/My Drive/Colab Notebooks/\" # You may need to change this, depending on where your notebooks are on Google Drive\n",
    "else:\n",
    "  base_dir = \".\"\n",
    "dataset_dir = os.path.join(base_dir, \"datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIWhqATw4oTQ"
   },
   "source": [
    "## Acknowledgments\n",
    "- The first two pieces of visualization code come from: F. Chollet: *Deep Learning with Python (2nd edn)*, Manning Publications, 2021\n",
    "- The third piece of visualization code is slightly modified from [Adrian Rosebrock's web site](https://pyimagesearch.com/2020/03/09/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning/).\n",
    "- The code for adding spurious correlations to MNIST is adpated from [https://github.com/dtak/rrr/blob/master/rrr/decoy_mnist.py](https://github.com/dtak/rrr/blob/master/rrr/decoy_mnist.py), which is the repo that accompanies the paper: Andrew Slavin Ross, Michael C. Hughes and Finale Doshi-Velez: *Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations*, Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, pp.2662-2670, 2017\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Ny6Y9Ja4oTR"
   },
   "source": [
    "## Visualizing Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9XhTKbJw7WZ9"
   },
   "source": [
    "- We'll use three visualizations to gain insight into what a network learns.\n",
    "- You do not have to understand the code!\n",
    "- We will run these visualizations on a convolutional neural network called Xception that has been pre-trained on the ImageNet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 876,
     "status": "ok",
     "timestamp": 1755852018078,
     "user": {
      "displayName": "Derek Bridge",
      "userId": "01707280039117110470"
     },
     "user_tz": -60
    },
    "id": "K3DdJBrp4oTR"
   },
   "outputs": [],
   "source": [
    "# In some cases, we'll just use the base\n",
    "xception_base = Xception(weights=\"imagenet\", include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 879,
     "status": "ok",
     "timestamp": 1755852018956,
     "user": {
      "displayName": "Derek Bridge",
      "userId": "01707280039117110470"
     },
     "user_tz": -60
    },
    "id": "tgexEXmb4oTR"
   },
   "outputs": [],
   "source": [
    "# In other cases, we'll use the top as well\n",
    "xception_model = Xception(weights=\"imagenet\", include_top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1755852019088,
     "user": {
      "displayName": "Derek Bridge",
      "userId": "01707280039117110470"
     },
     "user_tz": -60
    },
    "id": "s5ed7hS94oTR",
    "outputId": "74d270eb-50d1-4683-8d4a-c53997fc5a72"
   },
   "outputs": [],
   "source": [
    "xception_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2743,
     "status": "ok",
     "timestamp": 1755852021832,
     "user": {
      "displayName": "Derek Bridge",
      "userId": "01707280039117110470"
     },
     "user_tz": -60
    },
    "id": "JHik47zQ4oTR"
   },
   "outputs": [],
   "source": [
    "# We'll also make use of a cat image that we used in a previous lecture.\n",
    "img_path = os.path.join(dataset_dir, \"wikipedia_cats_and_dogs/Orange_tabby_cat_sitting_on_fallen_leaves-Hisashi-01A.jpg\")\n",
    "img = load_img(img_path, target_size=(299, 299))\n",
    "img_array = img_to_array(img)\n",
    "img_tensor = np.expand_dims(img_array, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "executionInfo": {
     "elapsed": 186,
     "status": "ok",
     "timestamp": 1755852022020,
     "user": {
      "displayName": "Derek Bridge",
      "userId": "01707280039117110470"
     },
     "user_tz": -60
    },
    "id": "i-8t_gRU-_PE",
    "outputId": "46165dcc-b671-4835-c3c2-9c45e4c6221f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(img_tensor[0].astype(\"uint8\"))\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljjTLQFH4oTR"
   },
   "source": [
    "## Visualizations of the activations of convolutional and pooling layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXjgsTORBtBo"
   },
   "source": [
    "Create a model that returns the activations of the convolutional and pooling layers of the Xception model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1755852022021,
     "user": {
      "displayName": "Derek Bridge",
      "userId": "01707280039117110470"
     },
     "user_tz": -60
    },
    "id": "Valp_wxw4oTR"
   },
   "outputs": [],
   "source": [
    "layer_outputs = []\n",
    "layer_names = []\n",
    "for layer in xception_base.layers:\n",
    "    if isinstance(layer, (Conv2D, MaxPooling2D)):\n",
    "        layer_outputs.append(layer.output)\n",
    "        layer_names.append(layer.name)\n",
    "activation_model = Model(xception_base.input, layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSzldbieBz_u"
   },
   "source": [
    "Feed our example image into the model in order to compute the layer activations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19241,
     "status": "ok",
     "timestamp": 1755852041259,
     "user": {
      "displayName": "Derek Bridge",
      "userId": "01707280039117110470"
     },
     "user_tz": -60
    },
    "id": "E5YkDyD-4oTR",
    "outputId": "a083b18a-ac81-4241-bee8-e03310ea67ed"
   },
   "outputs": [],
   "source": [
    "activations = activation_model.predict(img_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqwXRyF9CADE"
   },
   "source": [
    "Now we can plot the activations of the layers.\n",
    "\n",
    "E.g., here are the activations of feature map 1 in convolutional layer 0. What do you think this feature map detects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "executionInfo": {
     "elapsed": 222,
     "status": "ok",
     "timestamp": 1755852041479,
     "user": {
      "displayName": "Derek Bridge",
      "userId": "01707280039117110470"
     },
     "user_tz": -60
    },
    "id": "MoLV6sZg4oTS",
    "outputId": "b9795094-8a8d-486c-a868-78453fd6f8b9"
   },
   "outputs": [],
   "source": [
    "plt.matshow(activations[0][0, :, :, 1], cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzU7CFYGCJqd"
   },
   "source": [
    "E.g., here are the activations of the feature map 9 in convolutional layer 0.  What do you think this feature map detects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "executionInfo": {
     "elapsed": 141,
     "status": "ok",
     "timestamp": 1755852041619,
     "user": {
      "displayName": "Derek Bridge",
      "userId": "01707280039117110470"
     },
     "user_tz": -60
    },
    "id": "-0bvIw7_CQnh",
    "outputId": "0dbf6de1-50ae-43b2-c4b8-391a9247e65f"
   },
   "outputs": [],
   "source": [
    "plt.matshow(activations[0][0, :, :, 9], cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3Uhtl8JCo1Q"
   },
   "source": [
    " Now a visualization of all activations of all feature maps in all convolutional and pooling layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1Pl5kX7YVZy-PfPxwMJRabSW_XP5LgnOT"
    },
    "executionInfo": {
     "elapsed": 12335,
     "status": "ok",
     "timestamp": 1755852053951,
     "user": {
      "displayName": "Derek Bridge",
      "userId": "01707280039117110470"
     },
     "user_tz": -60
    },
    "id": "9xfHZMCW4oTS",
    "outputId": "6fd9c5c8-0001-4370-e585-00fdf18e5923"
   },
   "outputs": [],
   "source": [
    "images_per_row = 16\n",
    "\n",
    "# Iterate over the layers\n",
    "for layer_name, layer_activation in zip(layer_names, activations):\n",
    "    # This is the number of features in the feature map\n",
    "    n_features = layer_activation.shape[-1]\n",
    "\n",
    "    # The feature map has shape (1, size, size, n_features)\n",
    "    size = layer_activation.shape[1]\n",
    "\n",
    "    # We will tile the activation channels in this matrix\n",
    "    n_cols = n_features // images_per_row\n",
    "    display_grid = np.zeros(((size + 1) * n_cols -1,\n",
    "                             images_per_row * (size + 1) -1))\n",
    "\n",
    "    # We'll tile each filter into this big horizontal grid\n",
    "    for col in range(n_cols):\n",
    "        for row in range(images_per_row):\n",
    "            channel_index = col * images_per_row + row\n",
    "            channel_image = layer_activation[0, :, :, channel_index].copy()\n",
    "            # Post-process the feature to make it visually palatable\n",
    "            if channel_image.sum() != 0:\n",
    "                channel_image -= channel_image.mean()\n",
    "                channel_image /= channel_image.std()\n",
    "                channel_image *= 64\n",
    "                channel_image += 128\n",
    "            channel_image = np.clip(channel_image, 0, 255).astype(\"uint8\")\n",
    "            display_grid[\n",
    "                col * (size + 1): (col + 1) * size + col,\n",
    "                row * (size + 1): (row + 1) * size + row] = channel_image\n",
    "    # Display the grid\n",
    "    scale = 1. / size\n",
    "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
    "                        scale * display_grid.shape[0]))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(display_grid, aspect=\"auto\", cmap=\"viridis\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a56x45ma4oTS"
   },
   "source": [
    "- Lower layers are edge dectectors and, because these edges are common, there is a lot of activation.\n",
    "- Higher in the network, features become more abstract and hence activation is less about the image and more about the class.\n",
    "- In higher layers, there are cases of almost no activation, meaning the feature is not present at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdFkOf5Q4oTS"
   },
   "source": [
    "## Visualizations of the inputs that convolutional layers are receptive to\n",
    "\n",
    "In this visualization, we display the kinds of inputs that feature maps respond to. This is done by gradient ascent on the input space:\n",
    "- start from a blankish input image\n",
    "- find the changes to the input that maximise the response of a feature map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gkkVTXzLGvYA"
   },
   "source": [
    "We specify which layer we are interested in. You can change this to any of the other layers, e.g. \"block2_sepconv1\", \"block4_sepconv1\", \"block10_sepconv1\"...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1755852054133,
     "user": {
      "displayName": "Derek Bridge",
      "userId": "01707280039117110470"
     },
     "user_tz": -60
    },
    "id": "6-2Nq_cfGpVB"
   },
   "outputs": [],
   "source": [
    "layer_name = \"block2_sepconv1\"\n",
    "#layer_name = \"block4_sepconv1\"\n",
    "#layer_name = \"block10_sepconv1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1755852054136,
     "user": {
      "displayName": "Derek Bridge",
      "userId": "01707280039117110470"
     },
     "user_tz": -60
    },
    "id": "-pZUjQ9Z4oTS"
   },
   "outputs": [],
   "source": [
    "layer = xception_base.get_layer(name=layer_name)\n",
    "feature_extractor = Model(xception_base.input, layer.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1755852054138,
     "user": {
      "displayName": "Derek Bridge",
      "userId": "01707280039117110470"
     },
     "user_tz": -60
    },
    "id": "GWHfix3t4oTS"
   },
   "outputs": [],
   "source": [
    "def compute_loss(image, filter_index):\n",
    "    activation = feature_extractor(image)\n",
    "    filter_activation = activation[:, 2:-2, 2:-2, filter_index]\n",
    "    return tf.reduce_mean(filter_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1755852054140,
     "user": {
      "displayName": "Derek Bridge",
      "userId": "01707280039117110470"
     },
     "user_tz": -60
    },
    "id": "DTDq5Buw4oTS"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def gradient_ascent_step(image, filter_index, learning_rate):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(image)\n",
    "        loss = compute_loss(image, filter_index)\n",
    "    grads = tape.gradient(loss, image)\n",
    "    grads = tf.math.l2_normalize(grads)\n",
    "    image += learning_rate * grads\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1755852054143,
     "user": {
      "displayName": "Derek Bridge",
      "userId": "01707280039117110470"
     },
     "user_tz": -60
    },
    "id": "LHXUc7r74oTS"
   },
   "outputs": [],
   "source": [
    "img_width = 200\n",
    "img_height = 200\n",
    "\n",
    "def generate_filter_pattern(filter_index):\n",
    "    iterations = 30\n",
    "    learning_rate = 10.\n",
    "    image = tf.random.uniform(\n",
    "        minval=0.4,\n",
    "        maxval=0.6,\n",
    "        shape=(1, img_width, img_height, 3))\n",
    "    for i in range(iterations):\n",
    "        image = gradient_ascent_step(image, filter_index, learning_rate)\n",
    "    return image[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1755852054146,
     "user": {
      "displayName": "Derek Bridge",
      "userId": "01707280039117110470"
     },
     "user_tz": -60
    },
    "id": "c0_5IqKF4oTS"
   },
   "outputs": [],
   "source": [
    "def deprocess_image(image):\n",
    "    image -= image.mean()\n",
    "    image /= image.std()\n",
    "    image *= 64\n",
    "    image += 128\n",
    "    image = np.clip(image, 0, 255).astype(\"uint8\")\n",
    "    image = image[25:-25, 25:-25, :]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opfi9xYUH-PM"
   },
   "source": [
    "So here are the kinds of inputs that the second channel in layer block3_speconv1 responds to. What do you think it responds to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "executionInfo": {
     "elapsed": 1044,
     "status": "ok",
     "timestamp": 1755852055191,
     "user": {
      "displayName": "Derek Bridge",
      "userId": "01707280039117110470"
     },
     "user_tz": -60
    },
    "id": "-pxxkkeTHnGL",
    "outputId": "718c339f-1abc-42b6-c6f0-63f4fadeb536"
   },
   "outputs": [],
   "source": [
    "plt.imshow(deprocess_image(generate_filter_pattern(filter_index=2)))\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEVOsgKWIa8-"
   },
   "source": [
    "Now a visualization for every feature map  in the layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12199,
     "status": "ok",
     "timestamp": 1755852067392,
     "user": {
      "displayName": "Derek Bridge",
      "userId": "01707280039117110470"
     },
     "user_tz": -60
    },
    "id": "txLFpLZj4oTS"
   },
   "outputs": [],
   "source": [
    "all_images = []\n",
    "for filter_index in range(64):\n",
    "    image = deprocess_image(\n",
    "        generate_filter_pattern(filter_index)\n",
    "    )\n",
    "    all_images.append(image)\n",
    "\n",
    "margin = 5\n",
    "n = 8\n",
    "cropped_width = img_width - 25 * 2\n",
    "cropped_height = img_height - 25 * 2\n",
    "width = n * cropped_width + (n - 1) * margin\n",
    "height = n * cropped_height + (n - 1) * margin\n",
    "stitched_filters = np.zeros((width, height, 3))\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        image = all_images[i * n + j]\n",
    "        stitched_filters[\n",
    "            (cropped_width + margin) * i : (cropped_width + margin) * i + cropped_width,\n",
    "            (cropped_height + margin) * j : (cropped_height + margin) * j\n",
    "            + cropped_height,\n",
    "            :,\n",
    "        ] = image\n",
    "\n",
    "save_img(os.path.join(base_dir, f\"visualizations/filters_for_layer_{layer_name}.png\"), stitched_filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0who5TJ64oTS"
   },
   "source": [
    "- If you look at the images (saved in the `visualizations` folder), you'll see that the feature maps in lower layers, e.g. block2_sepconv1, respond to simple edges and colours.\n",
    "- The feature maps in slighlty later layers, e.g. block4_sepconv1, respond to simple textures made from combinations of edges and colours.\n",
    "- The feature maps in later layers respond to natural-looking textures resembling feathers, leaves, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72EUCbDR4oTS"
   },
   "source": [
    "## Visualizations of heatmaps that show parts of an image that most contribute to a classification.\n",
    "\n",
    "For a given input image and a predicted class, this will show which parts of the image were most useful in making the classification. This is sometimes called a **heatmap** or a **saliency map**.\n",
    "- For every pixel, we compute a score indicating how important that pixel is in predicting the class.\n",
    "- We display the scores as a heatmap.\n",
    "\n",
    "This can be helpful in debugging models: we can see whether the model is paying attention to the 'right' parts of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCAM:\n",
    "    def __init__(self, model, classIdx, layerName=None):\n",
    "        # store the model, the class index used to measure the class\n",
    "        # activation map, and the layer to be used when visualizing\n",
    "        # the class activation map\n",
    "        self.model = model\n",
    "        self.classIdx = classIdx\n",
    "        self.layerName = layerName\n",
    "        # if the layer name is None, attempt to automatically find\n",
    "        # the target output layer\n",
    "        if self.layerName is None:\n",
    "            self.layerName = self.find_target_layer()\n",
    "            \n",
    "    def find_target_layer(self):\n",
    "        # attempt to find the final convolutional layer in the network\n",
    "        # by looping over the layers of the network in reverse order\n",
    "        for layer in reversed(self.model.layers):\n",
    "            # check to see if the layer has a 4D output\n",
    "            if len(layer.output.shape) == 4:\n",
    "                return layer.name\n",
    "        # otherwise, we could not find a 4D layer so the GradCAM\n",
    "        # algorithm cannot be applied\n",
    "        raise ValueError(\"Could not find 4D layer. Cannot apply GradCAM.\")\n",
    "        \n",
    "    def compute_heatmap(self, image, eps=1e-8):\n",
    "        # construct our gradient model by supplying (1) the inputs\n",
    "        # to our pre-trained model, (2) the output of the (presumably)\n",
    "        # final 4D layer in the network, and (3) the output of the\n",
    "        # softmax activations from the model\n",
    "        gradModel = Model(\n",
    "            inputs=[self.model.inputs],\n",
    "            outputs=[self.model.get_layer(self.layerName).output,\n",
    "                     self.model.output])\n",
    "        # record operations for automatic differentiation\n",
    "        with tf.GradientTape() as tape:\n",
    "            # cast the image tensor to a float-32 data type, pass the\n",
    "            # image through the gradient model, and grab the loss\n",
    "            # associated with the specific class index\n",
    "            inputs = tf.cast(image, tf.float32)\n",
    "            (convOutputs, predictions) = gradModel(inputs)\n",
    "            loss = predictions[:, self.classIdx]\n",
    "        # use automatic differentiation to compute the gradients\n",
    "        grads = tape.gradient(loss, convOutputs)\n",
    "        # compute the guided gradients\n",
    "        castConvOutputs = tf.cast(convOutputs > 0, \"float32\")\n",
    "        castGrads = tf.cast(grads > 0, \"float32\")\n",
    "        guidedGrads = castConvOutputs * castGrads * grads\n",
    "        # the convolution and guided gradients have a batch dimension\n",
    "        # (which we don't need) so let's grab the volume itself and\n",
    "        # discard the batch\n",
    "        convOutputs = convOutputs[0]\n",
    "        guidedGrads = guidedGrads[0]\n",
    "        # compute the average of the gradient values, and using them\n",
    "        # as weights, compute the ponderation of the filters with\n",
    "        # respect to the weights\n",
    "        weights = tf.reduce_mean(guidedGrads, axis=(0, 1))\n",
    "        cam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-1)\n",
    "        # grab the spatial dimensions of the input image and resize\n",
    "        # the output class activation map to match the input image\n",
    "        # dimensions\n",
    "        (w, h) = (image.shape[2], image.shape[1])\n",
    "        heatmap = cv2.resize(cam.numpy(), (w, h))\n",
    "        # normalize the heatmap such that all values lie in the range\n",
    "        # [0, 1], scale the resulting values to the range [0, 255],\n",
    "        # and then convert to an unsigned 8-bit integer\n",
    "        numer = heatmap - np.min(heatmap)\n",
    "        denom = (heatmap.max() - heatmap.min()) + eps\n",
    "        heatmap = numer / denom\n",
    "        heatmap = (heatmap * 255).astype(\"uint8\")\n",
    "        # return the resulting heatmap to the calling function\n",
    "        return heatmap\n",
    "        \n",
    "    def overlay_heatmap(self, heatmap, image, alpha=0.5,\n",
    "                        colormap=cv2.COLORMAP_VIRIDIS):\n",
    "        # apply the supplied color map to the heatmap and then\n",
    "        # overlay the heatmap on the input image\n",
    "        heatmap = cv2.applyColorMap(heatmap, colormap)\n",
    "        output = image * alpha + heatmap * (1 - alpha)\n",
    "        output = output.astype(int)\n",
    "        # return a 2-tuple of the color mapped heatmap and the output,\n",
    "        # overlaid image\n",
    "        return (heatmap, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply it to our cat image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = xception_model.predict(img_tensor)\n",
    "i = np.argmax(preds[0])\n",
    "\n",
    "cam = GradCAM(xception_model, i)\n",
    "heatmap = cam.compute_heatmap(img_tensor)\n",
    "# resize the resulting heatmap to the original input image dimensions\n",
    "# and then overlay heatmap on top of the image\n",
    "heatmap = cv2.resize(heatmap, (img_array.shape[1], img_array.shape[0]))\n",
    "(heatmap, output) = cam.overlay_heatmap(heatmap, img_array, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpza6XfnGrYJ"
   },
   "source": [
    "## Shortcuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1755852069041,
     "user": {
      "displayName": "Derek Bridge",
      "userId": "01707280039117110470"
     },
     "user_tz": -60
    },
    "id": "YyS5r5hqO8dj"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def show_images(images):\n",
    "    num_images = len(images)\n",
    "    num_per_row = 5\n",
    "    num_rows = math.ceil(num_images / num_per_row)\n",
    "    fig, axes = plt.subplots(num_rows, num_per_row, figsize=(num_per_row, num_rows))\n",
    "    for i, image in enumerate(images):\n",
    "        r = i // num_per_row\n",
    "        c = i % num_per_row\n",
    "        ax = axes[c] if num_rows == 1 else axes[r, c]\n",
    "        ax.imshow(image, cmap=plt.cm.binary, interpolation=\"nearest\")\n",
    "        ax.axis(\"off\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1755852069044,
     "user": {
      "displayName": "Derek Bridge",
      "userId": "01707280039117110470"
     },
     "user_tz": -60
    },
    "id": "JB3L1QjZITid"
   },
   "outputs": [],
   "source": [
    "def augment_image(image, digit, mult=25):\n",
    "  img = image.copy()\n",
    "  fwd = [0,1,2,3]\n",
    "  rev = [-1,-2,-3,-4]\n",
    "  dir1 = fwd if np.random.rand() > 0.5 else rev\n",
    "  dir2 = fwd if np.random.rand() > 0.5 else rev\n",
    "  for i in dir1:\n",
    "    for j in dir2:\n",
    "      img[i][j] = 255 - mult * digit\n",
    "  return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1755852069046,
     "user": {
      "displayName": "Derek Bridge",
      "userId": "01707280039117110470"
     },
     "user_tz": -60
    },
    "id": "NoiDl5SVJLN4"
   },
   "outputs": [],
   "source": [
    "def augment_images(images, labels=None, mult=25):\n",
    "  digits = range(10)\n",
    "  l, h, w, d = images.shape\n",
    "  augmented_images = np.zeros(shape=(l, h, w, d))\n",
    "  for i in range(0, l):\n",
    "    augmented_images[i] = augment_image(images[i], np.random.choice(digits) if labels is None else labels[i])\n",
    "  return augmented_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1272,
     "status": "ok",
     "timestamp": 1755852070320,
     "user": {
      "displayName": "Derek Bridge",
      "userId": "01707280039117110470"
     },
     "user_tz": -60
    },
    "id": "YZjYW-PAJOpV",
    "outputId": "5db3a29f-e511-4e69-9d5c-3abffd8c7979"
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1755852070325,
     "user": {
      "displayName": "Derek Bridge",
      "userId": "01707280039117110470"
     },
     "user_tz": -60
    },
    "id": "ta4VLvDrMor9"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((60000, 28, 28, 1))\n",
    "X_test = X_test.reshape((10000, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1755852070356,
     "user": {
      "displayName": "Derek Bridge",
      "userId": "01707280039117110470"
     },
     "user_tz": -60
    },
    "id": "9smfHRiQJ1g-"
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(28, 28, 1))\n",
    "x = Rescaling(scale=1./255)(inputs)\n",
    "x = Conv2D(filters=64, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "outputs = Dense(units=10, activation=\"softmax\")(x)\n",
    "mnist_model = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1755852070399,
     "user": {
      "displayName": "Derek Bridge",
      "userId": "01707280039117110470"
     },
     "user_tz": -60
    },
    "id": "Wd63E5oJJ-U5"
   },
   "outputs": [],
   "source": [
    "mnist_model.compile(optimizer=RMSprop(learning_rate=0.0001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1070,
     "status": "ok",
     "timestamp": 1755852071467,
     "user": {
      "displayName": "Derek Bridge",
      "userId": "01707280039117110470"
     },
     "user_tz": -60
    },
    "id": "cLQHrCYhKEXc"
   },
   "outputs": [],
   "source": [
    "X_train_aug = augment_images(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "executionInfo": {
     "elapsed": 119,
     "status": "ok",
     "timestamp": 1755852071587,
     "user": {
      "displayName": "Derek Bridge",
      "userId": "01707280039117110470"
     },
     "user_tz": -60
    },
    "id": "xC4GYCZDKMcI",
    "outputId": "6dc0c8ca-6a1f-4cfd-dbde-798a430fb4f5"
   },
   "outputs": [],
   "source": [
    "show_images(X_train_aug[y_train == 3][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "executionInfo": {
     "elapsed": 80,
     "status": "ok",
     "timestamp": 1755852071666,
     "user": {
      "displayName": "Derek Bridge",
      "userId": "01707280039117110470"
     },
     "user_tz": -60
    },
    "id": "3mASgUOFPL7b",
    "outputId": "536a14c0-960e-409e-ddc9-91ee1eb716f0"
   },
   "outputs": [],
   "source": [
    "show_images(X_train_aug[y_train == 7][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 106978,
     "status": "ok",
     "timestamp": 1755852178643,
     "user": {
      "displayName": "Derek Bridge",
      "userId": "01707280039117110470"
     },
     "user_tz": -60
    },
    "id": "h5ylos_yKfQC",
    "outputId": "788dda09-4ccf-40d2-bbf9-f3785bf76b15"
   },
   "outputs": [],
   "source": [
    "mnist_model.fit(X_train_aug, y_train, epochs=20, batch_size=32, verbose=0, validation_split=0.25,\n",
    "                  callbacks=[EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 98,
     "status": "ok",
     "timestamp": 1755852178738,
     "user": {
      "displayName": "Derek Bridge",
      "userId": "01707280039117110470"
     },
     "user_tz": -60
    },
    "id": "V9opvrLTKgzP"
   },
   "outputs": [],
   "source": [
    "X_test_aug = augment_images(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1297,
     "status": "ok",
     "timestamp": 1755852180034,
     "user": {
      "displayName": "Derek Bridge",
      "userId": "01707280039117110470"
     },
     "user_tz": -60
    },
    "id": "ct3vCPUsKj8B",
    "outputId": "ef90f1ee-2a83-47e0-e9ec-b8a2d4b68a43"
   },
   "outputs": [],
   "source": [
    "mnist_model.evaluate(X_test_aug, y_test, verbose=0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 512,
     "status": "ok",
     "timestamp": 1755852180545,
     "user": {
      "displayName": "Derek Bridge",
      "userId": "01707280039117110470"
     },
     "user_tz": -60
    },
    "id": "Jx3ZBK86Kn5r"
   },
   "outputs": [],
   "source": [
    "X_test_aug_rand = augment_images(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "executionInfo": {
     "elapsed": 114,
     "status": "ok",
     "timestamp": 1755852180658,
     "user": {
      "displayName": "Derek Bridge",
      "userId": "01707280039117110470"
     },
     "user_tz": -60
    },
    "id": "KvJky-gRKrG5",
    "outputId": "386a49ca-9c9c-4039-b0ad-550cc5e16b7b"
   },
   "outputs": [],
   "source": [
    "show_images(X_test_aug_rand[y_test == 3][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1399,
     "status": "ok",
     "timestamp": 1755852182058,
     "user": {
      "displayName": "Derek Bridge",
      "userId": "01707280039117110470"
     },
     "user_tz": -60
    },
    "id": "D6kc6E8NKuSy",
    "outputId": "0ca4c9b2-ad23-4bcf-e947-56d444c66350"
   },
   "outputs": [],
   "source": [
    "mnist_model.evaluate(X_test_aug_rand, y_test, verbose=0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1342,
     "status": "ok",
     "timestamp": 1755855067327,
     "user": {
      "displayName": "Derek Bridge",
      "userId": "01707280039117110470"
     },
     "user_tz": -60
    },
    "id": "Adz1n2tpGncZ",
    "outputId": "249db7eb-d445-41b6-eb07-6d185ebec190"
   },
   "outputs": [],
   "source": [
    "correct = X_test_aug[np.argmax(mnist_model.predict(X_test_aug_rand), axis=1) == y_test]\n",
    "incorrect = X_test_aug[np.argmax(mnist_model.predict(X_test_aug_rand), axis=1) != y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1755856369860,
     "user": {
      "displayName": "Derek Bridge",
      "userId": "01707280039117110470"
     },
     "user_tz": -60
    },
    "id": "tlnFaeAlUIa7"
   },
   "outputs": [],
   "source": [
    "# Let's look at the fourth image it gets right and the fourth it gets wrong.\n",
    "img = correct[4]\n",
    "img_array = img_to_array(img)\n",
    "img_tensor = np.expand_dims(img_array, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1755856341095,
     "user": {
      "displayName": "Derek Bridge",
      "userId": "01707280039117110470"
     },
     "user_tz": -60
    },
    "id": "VKxs4WEDTWpu"
   },
   "outputs": [],
   "source": [
    "preds = mnist_model.predict(img_tensor)\n",
    "i = np.argmax(preds[0])\n",
    "\n",
    "cam = GradCAM(mnist_model, i)\n",
    "heatmap = cam.compute_heatmap(img_tensor)\n",
    "heatmap = cv2.resize(heatmap, (img_array.shape[1], img_array.shape[0]))\n",
    "(heatmap, output) = cam.overlay_heatmap(heatmap, img_array, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the fourth image it gets wrong.\n",
    "img = incorrect[4]\n",
    "img_array = img_to_array(img)\n",
    "img_tensor = np.expand_dims(img_array, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = mnist_model.predict(img_tensor)\n",
    "i = np.argmax(preds[0])\n",
    "\n",
    "cam = GradCAM(mnist_model, i)\n",
    "heatmap = cam.compute_heatmap(img_tensor)\n",
    "heatmap = cv2.resize(heatmap, (img_array.shape[1], img_array.shape[0]))\n",
    "(heatmap, output) = cam.overlay_heatmap(heatmap, img_array, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Discussion question:</b> Do you think any of these visualizations would be useful to someone who wanted to debug or audit a model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
